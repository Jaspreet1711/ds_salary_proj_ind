{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import Glassdoor_Scraper_V2 as gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New York City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages given by you of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "You will be required to download chromedriver.exe from the link mentioned below: \n",
      "https://chromedriver.chromium.org/downloads\n",
      "Copy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the Path (in code line 69 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Delhi\n",
      "Enter num of pages you want to scrape: 3\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet 2 Seconds are appropriate\n",
      "Enter Second(s) for page load time: 3\n",
      "Enter the Output file name you want to save: DS jobss\n",
      " \n",
      " \n",
      "\u001b[93mIt will take approximately 6 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "\u001b[92mOpening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Numbers_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-3\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n"
     ]
    },
    {
     "ename": "WebDriverException",
     "evalue": "Message: chrome not reachable\n  (Session info: chrome=94.0.4606.71)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7665219a1557>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgl_scrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\DS_Projects\\ds_sal_proj\\Glassdoor_Scraper_V2.py\u001b[0m in \u001b[0;36mgl_scrap\u001b[1;34m()\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mopening\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjobs_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mopening\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSecs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m1.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: chrome not reachable\n  (Session info: chrome=94.0.4606.71)\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Chicago\n",
      "Enter Num of Pages you want to scrape: 8\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Chicago\n",
      " \n",
      " \n",
      "It will take approximately 10 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-8\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-4 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-4\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-4\n",
      "\u001b[96mGetting_JobDescription_on_page-4_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-5 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-5\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-5\n",
      "\u001b[96mGetting_JobDescription_on_page-5_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-6 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-6\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-6\n",
      "\u001b[96mGetting_JobDescription_on_page-6_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-7 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-7\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-7\n",
      "\u001b[96mGetting_JobDescription_on_page-7_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-8 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-8\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-8\n",
      "\u001b[96mGetting_JobDescription_on_page-8_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_8_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Chicago with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Chicago.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Chicago.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bay Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Bay Area\n",
      "Enter Num of Pages you want to scrape: 8\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in BayArea\n",
      " \n",
      " \n",
      "It will take approximately 10 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-8\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-4 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-4\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-4\n",
      "\u001b[96mGetting_JobDescription_on_page-4_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-5 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-5\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-5\n",
      "\u001b[96mGetting_JobDescription_on_page-5_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-6 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-6\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-6\n",
      "\u001b[96mGetting_JobDescription_on_page-6_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-7 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-7\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-7\n",
      "\u001b[96mGetting_JobDescription_on_page-7_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-8 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-8\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-8\n",
      "\u001b[96mGetting_JobDescription_on_page-8_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_8_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in BayArea with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in BayArea.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in BayArea.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los Angsles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Los Angeles\n",
      "Enter Num of Pages you want to scrape: 5\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in LA\n",
      " \n",
      " \n",
      "It will take approximately 7 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-5\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-4 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-4\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-4\n",
      "\u001b[96mGetting_JobDescription_on_page-4_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-5 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-5\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-5\n",
      "\u001b[96mGetting_JobDescription_on_page-5_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_5_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in LA with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in LA.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in LA.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seattle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Seattle\n",
      "Enter Num of Pages you want to scrape: 8\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Seattle\n",
      " \n",
      " \n",
      "It will take approximately 10 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-8\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-4 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-4\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-4\n",
      "\u001b[96mGetting_JobDescription_on_page-4_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-5 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-5\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-5\n",
      "\u001b[96mGetting_JobDescription_on_page-5_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-6 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-6\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-6\n",
      "\u001b[96mGetting_JobDescription_on_page-6_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-7 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-7\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-7\n",
      "\u001b[96mGetting_JobDescription_on_page-7_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-8 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-8\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-8\n",
      "\u001b[96mGetting_JobDescription_on_page-8_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_8_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Seattle with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Seattle.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Seattle.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Houston\n",
      "Enter Num of Pages you want to scrape: 1\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Houston\n",
      " \n",
      " \n",
      "It will take approximately 3 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-1\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_1st_Page\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Houston with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Houston.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Houston.xlsx\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_1_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Houston with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Houston.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Houston.xlsx\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ddf71881e244>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglassdoor_scrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\DS_Projects\\ds_sal_proj\\Glassdoor_Scraper.py\u001b[0m in \u001b[0;36mglassdoor_scrape\u001b[1;34m()\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m     \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    686\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m         \"\"\"\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLOSE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m: Message: invalid session id\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dallas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Dallas\n",
      "Enter Num of Pages you want to scrape: 5\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Dallas\n",
      " \n",
      " \n",
      "It will take approximately 7 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-5\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-4 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-4\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-4\n",
      "\u001b[96mGetting_JobDescription_on_page-4_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-5 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-5\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-5\n",
      "\u001b[96mGetting_JobDescription_on_page-5_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_5_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Dallas with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Dallas.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Dallas.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Boston\n",
      "Enter Num of Pages you want to scrape: 8\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Boston\n",
      " \n",
      " \n",
      "It will take approximately 10 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-8\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-4 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-4\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-4\n",
      "\u001b[96mGetting_JobDescription_on_page-4_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-5 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-5\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-5\n",
      "\u001b[96mGetting_JobDescription_on_page-5_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-6 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-6\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-6\n",
      "\u001b[96mGetting_JobDescription_on_page-6_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-7 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-7\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-7\n",
      "\u001b[96mGetting_JobDescription_on_page-7_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-8 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-8\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-8\n",
      "\u001b[96mGetting_JobDescription_on_page-8_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_8_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Boston with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Boston.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Boston.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "philadelphia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Philadelphia\n",
      "Enter Num of Pages you want to scrape: 4\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Philadelphia\n",
      " \n",
      " \n",
      "It will take approximately 6 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-4\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-4 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-4\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-4\n",
      "\u001b[96mGetting_JobDescription_on_page-4_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_4_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Philadelphia with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Philadelphia.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Philadelphia.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Florida (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Florida\n",
      "Enter Num of Pages you want to scrape: 3\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Florida\n",
      " \n",
      " \n",
      "It will take approximately 5 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-3\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_3_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Florida with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Florida.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Florida.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colorado (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Colorado\n",
      "Enter Num of Pages you want to scrape: 2\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Colorado\n",
      " \n",
      " \n",
      "It will take approximately 4 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-2\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_2_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Colorado with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Colorado.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Colorado.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ohio (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Ohio\n",
      "Enter Num of Pages you want to scrape: 3\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Ohio\n",
      " \n",
      " \n",
      "It will take approximately 5 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-3\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_3_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Ohio with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Ohio.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Ohio.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "North Carolina (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: North Carolina\n",
      "Enter Num of Pages you want to scrape: 4\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in North Carolina\n",
      " \n",
      " \n",
      "It will take approximately 6 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-4\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-3 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-3\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-3\n",
      "\u001b[96mGetting_JobDescription_on_page-3_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-4 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-4\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-4\n",
      "\u001b[96mGetting_JobDescription_on_page-4_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_4_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in North Carolina with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in North Carolina.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in North Carolina.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arizona (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Arizona\n",
      "Enter Num of Pages you want to scrape: 2\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Arizona\n",
      " \n",
      " \n",
      "It will take approximately 4 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-2\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_2_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Arizona with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Arizona.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Arizona.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utah (State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThis function will fetch details from Glassdoor Website uptill pages you have entered of various openings of any role or job title within a location given by you.\n",
      "Details will be fetched in excel format.\n",
      " \n",
      "\u001b[92mYou will be required to download chromedriver.exe from the link mentioned below: \n",
      "\u001b[94mhttps://chromedriver.chromium.org/downloads\n",
      "\u001b[92mCopy & paste the downloaded chromedriver.exe in the same folder where you have saved this Glassdoor_Scraper.py file\n",
      "\u001b[93mChange the executable_path (in code line 78 & 634 in Glassdoor_Scraper.py) to your system path where you have saved the chromediver.exe\n",
      "\u001b[92mYour output excel will be saved in same folder.\n",
      " \n",
      "Also, make sure that all the libraries mentioned in starting of Glassdoor_Scraper.py are pip install in your system using Anaconda Prompt\n",
      " \n",
      " \n",
      "Enter the Job_title or Role: Data Scientist\n",
      "Enter the location: Utah\n",
      "Enter Num of Pages you want to scrape: 2\n",
      " \n",
      "Depending upon your Internet Speed decide second(s) to wait before page fully uploads\n",
      "3 Seconds are ideal. Go for 4 or 5 Seconds if your Internet Speed is really slow.\n",
      "If you have fast Internet(like 1-Gbps) 2 Seconds are appropriate.\n",
      "Enter Second(s) for page load time: 3\n",
      " \n",
      "Enter the Output file name you want to save: DS Jobs in Utah\n",
      " \n",
      " \n",
      "It will take approximately 4 mins to scrape through 5 pages with all details into dataframe\n",
      " \n",
      " \n",
      "Opening_the_Glassdoor_Website_in_ChromeBrowser\n",
      "\u001b[94mJob Search Tag - Glassdoor\n",
      "\u001b[92mSearching_for_Job_Entered_by_you\n",
      "\u001b[93mJob_is_Successfully_Searched\n",
      "\u001b[92mIt_will_search_Jobs_on_location_entered\n",
      "\u001b[1mWait_for_few_seconds_as_page_loading_takes_time_while_searching_jobs_on_location\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\u001b[1mFinding_Page_Changing_Elements_on_WebPage\n",
      "\u001b[92mIt_Will_first_Let_Login-Pop-up_Trigger_and_Close_it_for_smooth_Scraping_ahead\n",
      "\u001b[92mWaiting_for_Login_Pop-up\n",
      "\u001b[93mLogin_Pop_up_Closed_Successfully\n",
      "\u001b[93mPage_Numbers_Element_Found_Successfully\n",
      " \n",
      " \n",
      "\u001b[94mNow_Scraping_will_start_from_page-1_till_page-2\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_Page-1 - In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-1\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-1\n",
      "\u001b[96mGetting_JobDescription_on_page-1_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[92mCollecting_all_Job_Opening_Details_on_page-2 In Element Form\n",
      "\u001b[92mExtracting_Text_from_Elements_collected_in_Page-2\n",
      "\u001b[93mAll_Details_Retrieved_Successfully_from_page-2\n",
      "\u001b[96mGetting_JobDescription_on_page-2_Wait_for_Few_More_Seconds\n",
      " \n",
      "\u001b[93mScraping_Job_Done_for_all_2_Pages\n",
      " \n",
      " \n",
      "\u001b[92mArranging data in DataFrame\n",
      " \n",
      " \n",
      "\u001b[93mDataFrame is made successfully and named as DS Jobs in Utah with all the details.\n",
      "\u001b[93mOutput will be saved in excel format in your system.\n",
      " \n",
      " \n",
      "\u001b[93mDS Jobs in Utah.xlsx is Successfully Saved in your system.\n",
      "\u001b[93mCheck your folder with Excel File named - DS Jobs in Utah.xlsx\n"
     ]
    }
   ],
   "source": [
    "gs.gl_scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have Consolidated all of these Data in one excel file (DS Jobs in USA (Consolidated).xlsx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
